{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation of segmentation-models library for the loss function\n",
    "!pip install segmentation-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "#Standard imports\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import segmentation_models as sm\n",
    "\n",
    "#Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import image_dataset_from_directory, img_to_array, load_img, array_to_img, save_img, to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "This experiment will train the 4 different U-Nets on the retinal lesion data. The experiment will proceed in the following steps:\n",
    "\n",
    "<ol>\n",
    "<li>Downloading the images and annotations : since the networks will be trained on cloud platforms, the data will need to be downloaded from {my} computer</li>\n",
    "<li>Seperate the data into train-val-test set with proportions 0.7:0.15:0.15</li>\n",
    "<li>Create tf datasets for train, val, test data</li>\n",
    "<li>Train a model\n",
    "<ul>\n",
    "<li>For 25-35 epochs (depending on the time taken for one epoch)</li>\n",
    "<li>With the dice loss</li>\n",
    "<li>With model checkpoint callback</li>\n",
    "<li>With training log callback</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Save two versions of the model : \n",
    "<ul>\n",
    "<li>The best version</li>\n",
    "<li>Version after full epoch training</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Once all models are trained, produce evaluation output for all models.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Seperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationFileSeperator:\n",
    "\n",
    "    \"\"\"\n",
    "    This object copies image files and their corresponding segmentation maps into train, test and validation directories\n",
    "    target_image_path : The path to the directory containing the train, test, and validation directories for images\n",
    "    original_image_dir : The path to the directory containing the original images\n",
    "    target_segmap_path : The path to the directory containing the train, test, and validation directories for for the segmentation\n",
    "    original_segmap_dir : The path to the directory containing the original segmentation maps\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_image_path, original_image_dir, target_segmap_path, original_segmap_dir):\n",
    "        self.target_image_path = target_image_path\n",
    "        self.original_image_dir = original_image_dir \n",
    "        self.target_segmap_path = target_segmap_path\n",
    "        self.original_segmap_dir = original_segmap_dir\n",
    "        \n",
    "    def dataset_segregate(self):\n",
    "        \n",
    "        #This function is used to create empty train, test, and validation directories within the main directory\n",
    "        for typ in [\"Train\", \"Test\", \"Valid\"]:\n",
    "            path_1 = os.path.join(self.target_image_path, typ)\n",
    "            os.makedirs(path_1)\n",
    "\n",
    "            path_2 = os.path.join(self.target_segmap_path, typ)\n",
    "            os.makedirs(path_2)\n",
    "    \n",
    "    def class_maker(self):\n",
    "        \n",
    "        #This function makes a directories within the train, test, and validation directories for each class in the datset.\n",
    "        for typ in os.listdir(self.target_image_path):\n",
    "            for section in os.listdir(self.original_image_dir):\n",
    "                path_1 = os.path.join(self.target_image_path, typ, section)\n",
    "                path_2 = os.path.join(self.target_segmap_path, typ, section)\n",
    "                os.makedirs(path_1)\n",
    "                os.makedirs(path_2)\n",
    "    \n",
    "    def shuffle_together(self, x,  y):\n",
    "        z = list(zip(x, y))\n",
    "        random.Random(9).shuffle(z)\n",
    "        a, b = zip(*z)\n",
    "        return a, b\n",
    "\n",
    "                \n",
    "    def file_mover(self, train_pr, valid_pr):\n",
    "        \n",
    "        #This function is the most important function. This moves all the files from the origianal directory into the target directories with the proportions for train, test and validation data.\n",
    "        #Section represents class\n",
    "        #dir represents train,test or val\n",
    "        for dir in os.listdir(self.target_image_path):\n",
    "            print(f\"Moving to dir: {dir}\")\n",
    "\n",
    "            d_path = self.original_image_dir\n",
    "            m_path = self.original_segmap_dir\n",
    "            if dir == \"Train\":\n",
    "                start_point = 0\n",
    "                cutoff = len(os.listdir(d_path)) - 1\n",
    "                end_point = int(train_pr * cutoff)\n",
    "            elif dir == \"Valid\":\n",
    "                start_off = len(os.listdir(d_path)) -1 \n",
    "                start_point = int(train_pr * start_off)\n",
    "\n",
    "                cutoff = len(os.listdir(d_path)) - 1\n",
    "                end_point = start_point + int(valid_pr * cutoff)\n",
    "            else:\n",
    "                test_pr = train_pr + valid_pr\n",
    "                start_off = len(os.listdir(d_path)) -1\n",
    "                start_point = int(test_pr * start_off)\n",
    "\n",
    "                   \n",
    "                end_point = len(os.listdir(d_path)) -1\n",
    "        \n",
    "            moveables_1 = sorted(os.listdir(d_path))\n",
    "            moveables_2 = sorted(os.listdir(m_path))\n",
    "            mv_1, mv_2 = self.shuffle_together(moveables_1, moveables_2)\n",
    "            for i in range(start_point, end_point):\n",
    "                try:\n",
    "                    i_src_path = os.path.join(self.original_image_dir, mv_1[i])\n",
    "                    i_des_path = os.path.join(self.target_image_path, dir, mv_1[i])\n",
    "\n",
    "                    shutil.copy(i_src_path, i_des_path)\n",
    "\n",
    "                    #Moving segmaps\n",
    "                    m_src_path = os.path.join(self.original_segmap_dir, mv_2[i])\n",
    "                    m_des_path = os.path.join(self.target_segmap_path, dir, mv_2[i])\n",
    "\n",
    "                    shutil.copy(m_src_path, m_des_path)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    def print_statistics(self):\n",
    "        \n",
    "        for dir in os.listdir(self.target_image_path):\n",
    "    \n",
    "            num = len(os.listdir(f\"{self.target_image_path}/{dir}\"))\n",
    "            print(f\"Files in {dir} Directory -> : {num}\")\n",
    "            print(\" \")\n",
    "\n",
    "        \n",
    "        for dir in os.listdir(self.target_segmap_path):\n",
    "    \n",
    "\n",
    "            num = len(os.listdir(f\"{self.target_segmap_path}/{dir}\"))\n",
    "            print(f\"Files in {dir} Directory -> : {num}\")\n",
    "            print(\" \")\n",
    "            \n",
    "    \n",
    "    def run(self, train_pr: float, valid_pr: float):\n",
    "        \n",
    "        self.dataset_segregate()\n",
    "        #self.class_maker()\n",
    "\n",
    "        if train_pr < 0 or train_pr > 1:\n",
    "            print(\"Train proportion value not valid. Please enter a value greater than 0 and less than 1.\")\n",
    "        elif valid_pr < 0 or valid_pr > 1:\n",
    "            print(\"The validation proportion is not valid. Please enter a value greater than 0 and less than 1.\")\n",
    "        elif valid_pr > train_pr:\n",
    "            print(\"Validation proportion is greater than training data proportion, please enter a value less than the training proportion.\")\n",
    "        elif valid_pr + train_pr >= 1:\n",
    "            print(\"The sum of the validation and training proportion is greater than or equal to one, this is not valid. Please enter values such that their sum is strictly less than 1.\")\n",
    "\n",
    "        \n",
    "        self.file_mover(train_pr=train_pr, valid_pr=valid_pr)\n",
    "    def test_proportions(self, train_pr: float, valid_pr: float):\n",
    "\n",
    "        if train_pr < 0 or train_pr > 1:\n",
    "            print(\"Train proportion value not valid. Please enter a value greater than 0 and less than 1.\")\n",
    "        elif valid_pr < 0 or valid_pr > 1:\n",
    "            print(\"The validation proportion is not valid. Please enter a value greater than 0 and less than 1.\")\n",
    "        elif valid_pr > train_pr:\n",
    "            print(\"Validation proportion is greater than training data proportion, please enter a value less than the training proportion.\")\n",
    "        elif valid_pr + train_pr >= 1:\n",
    "            print(\"The sum of the validation and training proportion is greater than or equal to one, this is not valid. Please enter values such that their sum is strictly less than 1.\")\n",
    "        else:\n",
    "            print(\"The proportions enterred are valid.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF Dataset creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataSet:\n",
    "\n",
    "  \"\"\"\n",
    "  This object returns a zipped tf Dataset containing images and their corresponding segmentation maps\n",
    "  img_dir : The directory containing the images\n",
    "  maps_dir : The directory containing the segmentation maps\n",
    "  input_size : A tuple which specifies the height and width of the images and segmentation maps (h,w)\n",
    "  n_classes : The number of classes (including the background)\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, img_dir:str, maps_dir:str, input_size:tuple, n_classes:int):\n",
    "\n",
    "    self.img_root = pathlib.Path(img_dir)\n",
    "    self.map_root = pathlib.Path(maps_dir)\n",
    "    self.h , self.w = input_size\n",
    "    self.n = n_classes\n",
    "    self.img_root = str(self.img_root/'*/*')\n",
    "    self.map_root = str(self.map_root/'*/*')\n",
    "\n",
    "  def process_image(self,file_path):\n",
    "    file_name = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_jpeg(file_name, channels=3)\n",
    "    img = tf.image.resize(img, [self.h, self.w])\n",
    "    \n",
    "    return img\n",
    "\n",
    "  def process_annotation(self,file_path):\n",
    "\n",
    "    file_name = tf.io.read_file(file_path)\n",
    "    img = tf.io.decode_png(file_name, dtype=\"uint8\")\n",
    "    #img = img - 1\n",
    "    img = tf.image.resize(img, [self.h, self.w])\n",
    "    img = tf.cast(img, dtype=tf.int32)\n",
    "    img = tf.one_hot(img, self.n, axis=2)\n",
    "    return img\n",
    "\n",
    "  def create_segmentation_dataset(self):\n",
    "\n",
    "    seed = 5\n",
    "\n",
    "    image_ds = tf.data.Dataset.list_files(self.img_root, seed=seed)\n",
    "    ann_ds = tf.data.Dataset.list_files(self.map_root, seed=seed)\n",
    "\n",
    "    image_ds = image_ds.map(self.process_image)\n",
    "    ann_ds = ann_ds.map(self.process_annotation)\n",
    "    ds = tf.data.Dataset.zip((image_ds, ann_ds))\n",
    "    ds = ds.shuffle(1000, seed=seed).batch(32)\n",
    "\n",
    "    return ds\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callback which plots the training loss curve after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossPlot(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, path):\n",
    "        \n",
    "        super(LossPlot, self).__init__()\n",
    "        self.path = path\n",
    "        _, self.ax = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "\n",
    "        self.epochs = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch,logs=None):\n",
    "\n",
    "        self.epochs = self.epochs + 1\n",
    "\n",
    "    def build_plots(self, epochs, ax, y, y2):\n",
    "        ax.plot(range(epochs), y, \"b--\", label=\"Training Loss\")\n",
    "        ax.plot(range(epochs), y2, color=\"darkturquoise\", label=\"Validation Loss\")\n",
    "        ax.set_ylabel(\"loss\")\n",
    "        ax.set_xlabel(\"epochs\")\n",
    "        ax.legend()\n",
    "        plt.savefig(self.path, dpi=300)\n",
    "    \n",
    "    def on_train_end(self, logs):\n",
    "\n",
    "        y = logs[\"loss\"]\n",
    "        y2 = logs[\"val_loss\"]\n",
    "\n",
    "        self.build_plots(self.epochs, self.ax,y, y2)  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class EvalPipeline:\n",
    "    \n",
    "    def __init__(self, gt, img, n,model_dict, class_dict):\n",
    "        \n",
    "        self.gt = gt\n",
    "        self.img = img\n",
    "        self.model_dict = model_dict\n",
    "        self.n = n\n",
    "        self.class_dict = class_dict\n",
    "\n",
    "        self.prediction_gen()\n",
    "        \n",
    "    def prediction_gen(self):\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        for m in self.model_dict.keys():\n",
    "            model = self.model_dict[m]\n",
    "   \n",
    "            pred = model.predict(self.img)\n",
    "            pred = np.array(pred)\n",
    "            pred = np.argmax(pred, axis=3)\n",
    "            pred = pred.reshape(pred.shape[0], pred.shape[1], pred.shape[2], 1)\n",
    "            \n",
    "            predictions[m] = pred\n",
    "            \n",
    "        self.pred = predictions\n",
    "            \n",
    "    \n",
    "    def stage_one(self, metrics=[\"SENS\", \"SPEC\", \"IoU\", \"DSC\"], path=\"stage_1.csv\"):\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        #First evaluate the same metric for all sets of predictions\n",
    "        print(metrics)\n",
    "        for metric in metrics:\n",
    "            \n",
    "            scores[metric] = []\n",
    "            for p in self.pred.keys():\n",
    "                \n",
    "                \n",
    "                current_pred = self.pred[p]\n",
    "                #Evaluate for the current prediction\n",
    "                score = evaluate(self.gt, current_pred, metric=metric, multi_class=True, n_classes=self.n)\n",
    "                score = np.mean(score)\n",
    "                scores[metric].append(score)\n",
    "                \n",
    "        with open(path, \"w\") as f:\n",
    "            for m in metrics:\n",
    "                f.write(m)\n",
    "                f.write(\",\")\n",
    "            f.write(\"\\n\") \n",
    "            for i in range(len(scores[m])):\n",
    "                for m in metrics:\n",
    "                \n",
    "                    f.write(str(scores[m][i]))\n",
    "                    f.write(\",\")\n",
    "                f.write(\"\\n\")\n",
    "            \n",
    "            \n",
    "        return scores\n",
    "    \n",
    "    def stage_two(self, metrics=[\"SENS\", \"SPEC\", \"IoU\", \"DSC\"], path=\"stage_2.json\"):\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        # Have to change all of done_pred instances into self.pred\n",
    "        for p in self.pred.keys():\n",
    "            scores[p] = []\n",
    "            print(f\"Working on : {p}...\")\n",
    "            for metric in metrics:\n",
    "                \n",
    "                current_pred = self.pred[p]\n",
    "                #Evaluate for the current prediction\n",
    "                score = evaluate(self.gt, current_pred, metric=metric, multi_class=True, n_classes=self.n)\n",
    "                scores[p].append(score)\n",
    "            \n",
    "            scores[p] = np.array(scores[p]).T\n",
    "        \n",
    "        scores_2 = {}\n",
    "        \n",
    "        print(\"Creating final dict\")\n",
    "        for p in self.pred.keys():\n",
    "            scores_2[p] = {}\n",
    "            for i, c in enumerate(self.class_dict):\n",
    "                scores_2[p][c] = {}\n",
    "                for n, m in enumerate(metrics):\n",
    "                    scores_2[p][c][m] = scores[p][i][n]\n",
    "        \n",
    "        \n",
    "\n",
    "        with open(path, \"w\") as json_file:\n",
    "            json.dump(scores_2, json_file)\n",
    "        \n",
    "        \n",
    "        return scores_2\n",
    "\n",
    "    \n",
    "    def stage_four(self, img_dir, gt_dir, img_files, gt_files, path=\"stage_4.png\"):\n",
    "        \n",
    "        #Assumes the images aren't 4 dimensional tensors\n",
    "        \n",
    "        #Number of images should be the same as G.T\n",
    "        print(img_dir)\n",
    "        print(gt_dir)\n",
    "        print(img_files)\n",
    "        print(gt_files)\n",
    "        \n",
    "        n_cols = len(img_files)\n",
    "        n_rows = len(list(self.model_dict.keys())) +2\n",
    "        \n",
    "        #Defining the figure\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=(n_cols*3,n_rows*3))\n",
    "        \n",
    "        ax[0, 0].set_ylabel(\"images\")\n",
    "        ax[1, 0].set_ylabel(\"ground_truth\")\n",
    "        \n",
    "        #Plotting the images first\n",
    "        for i, img in enumerate(img_files):\n",
    "            \n",
    "            img_path = os.path.join(img_dir, img)\n",
    "            image = load_img(img_path)\n",
    "            ax[0, i].imshow(image)\n",
    "            \n",
    "            ax[0,i].set_xticks([])\n",
    "            ax[0, i].set_yticks([])\n",
    "        \n",
    "        #Plotting the ground truth\n",
    "        for i, ann in enumerate(gt_files):\n",
    "            \n",
    "            img_path = os.path.join(gt_dir, ann)\n",
    "            image = tf.io.read_file(img_path)\n",
    "            image = tf.io.decode_png(image)\n",
    "\n",
    "            ax[1, i].imshow(image)\n",
    "            \n",
    "            ax[1,i].set_xticks([])\n",
    "            ax[1, i].set_yticks([])\n",
    "        \n",
    "        for i, model_name in enumerate(self.model_dict.keys()):\n",
    "            \n",
    "            i = i+2\n",
    "            model = self.model_dict[model_name]\n",
    "            ax[i, 0].set_ylabel(model_name)\n",
    "            \n",
    "            for n, img in enumerate(img_files):\n",
    "                \n",
    "                img_path = os.path.join(img_dir, img)\n",
    "                image = tf.io.read_file(img_path)\n",
    "                image = tf.io.decode_jpeg(image, 3)\n",
    "                image = tf.image.resize(image, [224, 224])\n",
    "                image = np.expand_dims(image, axis=0)\n",
    "                pred = model.predict(image)\n",
    "                pred = np.squeeze(pred)\n",
    "                \n",
    "                ax[i, n].imshow(pred)\n",
    "                ax[i,n].set_xticks([])\n",
    "                ax[i,n].set_yticks([])\n",
    "            \n",
    "        plt.subplots_adjust(left=0.2,\n",
    "                    right=0.9,\n",
    "                    wspace=0.4,\n",
    "                    hspace=0.4)\n",
    "            \n",
    "        plt.savefig(path, dpi=100)\n",
    "        \n",
    "    \n",
    "    ### Stage 5 : F1 score plots per model per class\n",
    "    \n",
    "    def stage_five(self, path=\"stage_5.png\"):\n",
    "        \n",
    "\n",
    "        \n",
    "        markers = [\"x\", \"+\", \".\", \"1\", \"*\", \"d\"]\n",
    "        colors = [\"lime\", \"fuchsia\", \"darkorange\", \"gold\", \"salmon\", \"indigo\"]\n",
    "        \n",
    "        model_names = list(self.pred.keys())\n",
    "        \n",
    "        fig, ax = plt.subplots(1,1, figsize=(6,self.n))\n",
    "        ys = range(self.n)\n",
    "        \n",
    "        for i, model_name in enumerate(model_names):\n",
    "            \n",
    "            pred = self.pred[model_name]\n",
    "            \n",
    "            prec = evaluate(self.gt, pred, metric=\"PREC\", multi_class=True, n_classes=self.n)\n",
    "            recall = evaluate(self.gt, pred, metric=\"Recall\", multi_class=True, n_classes=self.n)\n",
    "            \n",
    "            f1 = (2 * prec * recall) / (prec + recall)\n",
    "            \n",
    "            some_num = np.random.uniform(0.1, 0.3, 1)\n",
    "            \n",
    "            ax.scatter(x=f1, y=ys, color=colors[i], marker=markers[i], label=model_name)\n",
    "            ax.set_yticks(ticks=[0,1,2],labels=self.class_dict)\n",
    "            ax.legend(loc=\"best\")\n",
    "            ax.grid()\n",
    "        \n",
    "        plt.savefig(path, dpi=100)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models : Encoders and decoders\n",
    "\n",
    "The following section of the code contains the different types of U-Nets that will be trained. The networks will be trained across different cloud platforms, where each platform will only train one network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weighted dice_loss -> background class is given a smaller weight\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=[0.25, 1.5, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder \n",
    "\n",
    "All the U-nets use the same decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SpatialAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.alpha = tf.Variable(initial_value=0.0, trainable=True)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.C = input_shape[-1]\n",
    "        self.H = input_shape[1]\n",
    "        self.W = input_shape[2]\n",
    "\n",
    "        #Defining the convolutions\n",
    "        self.conv1 = tf.keras.layers.Conv2D(self.C, 1)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(self.C, 1)\n",
    "        self.conv3 = tf.keras.layers.Conv2D(self.C, 1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        n_shape = self.H * self.W\n",
    "\n",
    "        a = inputs\n",
    "        b = self.conv1(inputs)\n",
    "        c = self.conv2(inputs)\n",
    "        d = self.conv3(inputs)\n",
    "\n",
    "        b = tf.transpose(tf.keras.layers.Reshape((n_shape, self.C))(b), perm=[0,2,1])\n",
    "        c = tf.keras.layers.Reshape((n_shape, self.C))(c)\n",
    "        d = tf.keras.layers.Reshape((n_shape, self.C))(d)\n",
    "\n",
    "        c = tf.linalg.matmul(c, b)\n",
    "        S = tf.keras.layers.Softmax()(c)\n",
    "        S = tf.transpose(S, perm=[0,2,1])\n",
    "\n",
    "        d = self.alpha * tf.linalg.matmul(S, d)\n",
    "        d = tf.keras.layers.Reshape((self.H, self.W, self.C))(d)\n",
    "        E = tf.keras.layers.Add()([a, d])        \n",
    "\n",
    "        return E\n",
    "\n",
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.beta = tf.Variable(initial_value=0.0, name=\"beta\", trainable=True)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.C = input_shape[-1]\n",
    "        self.H = input_shape[1]\n",
    "        self.W = input_shape[2]\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        a1=a2=a3=a4= inputs\n",
    "        n_shape = self.H * self.W\n",
    "        a2 = tf.keras.layers.Reshape((n_shape, self.C))(a2)\n",
    "        a3 = tf.keras.layers.Reshape((n_shape, self.C))(a3)\n",
    "        a4 = tf.transpose(tf.keras.layers.Reshape((n_shape, self.C))(a4), perm=[0,2,1])\n",
    "\n",
    "\n",
    "        #Creating X, the softmax on the matrix product of A_T_A\n",
    "        a_T_a = tf.linalg.matmul(a4, a3)\n",
    "        x = tf.keras.layers.Softmax()(a_T_a)\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "\n",
    "        a2_pass = self.beta * tf.linalg.matmul(a2, x)\n",
    "        a2_pass = tf.keras.layers.Reshape((self.H,self.W,self.C))(a2_pass)\n",
    "\n",
    "        E = tf.keras.layers.Add()([a1, a2_pass])\n",
    "\n",
    "        return E\n",
    "            \n",
    "\n",
    "class DualAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DualAttention, self).__init__()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.C = input_shape[-1]\n",
    "        self.conv1 = tf.keras.layers.Conv2D(self.C, 1)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(self.C, 1)\n",
    "        self.sam = SpatialAttention()\n",
    "        self.cam = ChannelAttention()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "\n",
    "        e1 = self.sam(inputs)\n",
    "        e2 = self.cam(inputs)\n",
    "\n",
    "        e1 = self.conv1(e1)\n",
    "        e2 = self.conv2(e2)\n",
    "\n",
    "        F = tf.keras.layers.Add()([e1, e2])\n",
    "        return F\n",
    "\n",
    "\n",
    "\n",
    "def decoder_block(a, x, f, attention=False):\n",
    "\n",
    "    if attention:\n",
    "        a = DualAttention()(a)\n",
    "\n",
    "    x = tf.keras.layers.Conv2DTranspose(filters=f, kernel_size=2, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    if a is not  None:\n",
    "        x = tf.concat([a, x], axis=-1)\n",
    "    x = tf.keras.layers.Conv2D(f, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Conv2D(f, 3, padding=\"same\", activation=\"relu\")(x) \n",
    "\n",
    "    return x\n",
    "\n",
    "def decoder_full(activations, x, filters, num_classes, attention_indices):\n",
    "\n",
    "    #Looping over the activations and filters from bottom to top\n",
    "    #Activation are reversed for this effect\n",
    " \n",
    "    \n",
    "    ai = None\n",
    "    #Flag to indicate whether the point of attention is found\n",
    "    found = True\n",
    "    #Flag to pass to the decoder block, whether dual attention should be applied\n",
    "    att = False\n",
    "    for i,(a,f) in enumerate(zip(activations[::-1],filters)):\n",
    "\n",
    "        #Flag to indicate whether there is no need for attention\n",
    "        there = len(attention_indices)\n",
    "        if found and there:\n",
    "            ai = attention_indices.pop()\n",
    "        \n",
    "        #Check if the current activation needs attention\n",
    "\n",
    "        att=found = (i+1 == ai)\n",
    "        print(att)\n",
    "        x = decoder_block(a, x, f, att)\n",
    "    \n",
    "    output = tf.keras.layers.Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(x)\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base U-Net\n",
    "\n",
    "This the Base U-Net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_block(inp,f):\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(f, 3, 1, activation=\"relu\",padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(f, 3, 1, activation=\"relu\",padding=\"same\")(x)\n",
    "    a = x\n",
    "    x = tf.keras.layers.MaxPool2D(2, 2)(x)\n",
    "\n",
    "    return x,a\n",
    "\n",
    "def last_encoder(inp, f):\n",
    "\n",
    "    x = inp\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(f, 3, 1, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = tf.keras.layers.Conv2D(f, 3, 1, activation=\"relu\", padding=\"same\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_unet(inp, filters):\n",
    "\n",
    "    activations = []\n",
    "\n",
    "    x = inp\n",
    "    for f in filters[:-1]:\n",
    "        x, a = encoder_block(x, f)\n",
    "        activations.append(a)\n",
    "    \n",
    "    x = last_encoder(x, filters[-1])\n",
    "    return x, activations\n",
    "\n",
    "def unet(num_classes, input_size, input_dim, att_indices=[], last_attention=False):\n",
    "\n",
    "    inp = tf.keras.layers.Input((input_size, input_size, input_dim))\n",
    "\n",
    "    filters = [64, 128, 256, 512, 1024]\n",
    "\n",
    "    x, a = encoder_unet(inp, filters=filters)\n",
    "\n",
    "    if last_attention:\n",
    "        x = DualAttention()(x)\n",
    "\n",
    "    output = decoder_full(a, x, filters[:-1][::-1], num_classes, att_indices)\n",
    "\n",
    "    model = tf.keras.Model(inp, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "unet_model = unet(5, 512, 3)\n",
    "unet_model.compile(optimizer=\"adam\", loss=dice_loss, metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "def vgg_encoder_block(x, layers):\n",
    "    \"\"\"\n",
    "    This function passes an input through a set of conv layers from VGG19, returning the downsampled and convolved activation\n",
    "    \"\"\"\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    addition = x\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2), strides = 2)(x)\n",
    "    return (x, addition)\n",
    "\n",
    "def last_vgg_block(x, layers):\n",
    "\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def vgg_encoder_full(input, layer_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates the full encoder given a dictionary of layers from the VGG network, it returns the final activation \n",
    "    and a list of intermediate activations\n",
    "    \"\"\"\n",
    "\n",
    "    activations = []\n",
    "    x = input\n",
    "    for layer_name in list(layer_dict.keys())[:-1]:\n",
    "        x, a = vgg_encoder_block(x, layer_dict[layer_name])\n",
    "        activations.append(a)\n",
    "    \n",
    "    x = last_vgg_block(x, layer_dict[list(layer_dict.keys())[-1]])\n",
    "    \n",
    "    return x, activations\n",
    "\n",
    "\n",
    "\n",
    "def vgg_unet(num_classes, input_size, input_dim, att_indices=[], last_attention=False):\n",
    "\n",
    "    #Downloading the VGG network\n",
    "    vgg19 = VGG19(weights=\"imagenet\", include_top=False, input_shape=(input_size, input_size,input_dim))\n",
    "    vgg19.trainable = False\n",
    "    #Getting all the blocks from the VGG network\n",
    "    vgg_blocks = {\n",
    "        f\"block{n}\" : [layer for layer in vgg19.layers if f\"block{n}_conv\" in layer.name] for n in range(1, 6)\n",
    "    }\n",
    "    \n",
    "    #Filters for the Decoder\n",
    "    filters = [512, 256, 128, 64]\n",
    "\n",
    "    l = len(att_indices)\n",
    "    assert l >= 0, \"Attention indices should be 0 or greater\"\n",
    "    assert l <= len(filters) + 1, \"Number of layers for attetention can not exceed 5\"\n",
    "\n",
    "    #assert len(att_indices[att_indices > 5]) == 0, \"Attention indices must be from 1 to 5\"\n",
    "    vgg_input = vgg19.input\n",
    "    #Defining the encoder\n",
    "    #First Preprocess the input\n",
    "    x = preprocess_input(x=vgg_input)\n",
    "    \n",
    "    x, a = vgg_encoder_full(x, vgg_blocks)\n",
    "\n",
    "    if last_attention:\n",
    "        x = DualAttention()(x)\n",
    "\n",
    "    output = decoder_full(a, x, filters, num_classes, att_indices)\n",
    "\n",
    "    vgg_unet_model = tf.keras.Model(vgg_input, output)\n",
    "\n",
    "    return vgg_unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "vgg_unet = vgg_unet(5, 512, 3)\n",
    "vgg_unet.compile(optimizer =\"adam\", loss=dice_loss, metrics =\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet50, preprocess_input\n",
    "\n",
    "def resblock_stem(input, layers):\n",
    "\n",
    "    x = input\n",
    "    for layer in layers:\n",
    "\n",
    "        x = layer(x)\n",
    "    \n",
    "    a = x\n",
    "    x = tf.keras.layers.MaxPool2D(2, 2)(x)\n",
    "\n",
    "    return x, a\n",
    "\n",
    "\n",
    "def resblock_enc(inp, layers, res):\n",
    "\n",
    "    enc = tf.keras.Model(inputs=res.get_layer(layers[0].name).input, outputs=res.get_layer(layers[-1].name).output)\n",
    "    x = enc(inp)\n",
    "    return x\n",
    "\n",
    "def resnet_encoder(inp, layer_dict, res):\n",
    "\n",
    "    keys = list(layer_dict.keys())\n",
    "\n",
    "    x, a = resblock_stem(inp, layer_dict[keys[0]])\n",
    "\n",
    "    activations = [a]\n",
    "    for block in keys[1:]:\n",
    "        x = resblock_enc(x, layer_dict[block], res) \n",
    "        activations.append(x)\n",
    "    \n",
    "    return x, [None] + activations[:-1]\n",
    "\n",
    "\n",
    "\n",
    "def resnet_unet(num_classes, input_size, input_dim, att_indices=[], last_attention=False):\n",
    "\n",
    "    #Downloading the ResNet\n",
    "    resnet = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(input_size,input_size,input_dim))\n",
    "\n",
    "    layer_dict = resnet_blocks = {\n",
    "    f\"block_{n}\" : [layer for layer in resnet.layers if layer.name[:5] == f\"conv{n}\"] for n in range(1, 5)\n",
    "    }\n",
    "\n",
    "    #Freezing the layers of the ResNet\n",
    "    resnet.trainable = False\n",
    "\n",
    "    #Building the model\n",
    "    inp = resnet.input\n",
    "    x = preprocess_input(x=inp)\n",
    "    x, a = resnet_encoder(inp, layer_dict, resnet)\n",
    "\n",
    "\n",
    "    #Decoder\n",
    "    filters = [512, 256, 128, 64]\n",
    "\n",
    "    l = len(att_indices)\n",
    "    assert l >= 0, \"Attention indices should be 0 or greater\"\n",
    "    assert l <= len(filters) + 1, \"Number of layers for attetention can not exceed 5\"\n",
    "\n",
    "    #assert len(att_indices[att_indices > 5]) == 0, \"Attention indices must be from 1 to 5\"\n",
    "\n",
    "    if last_attention:\n",
    "        x = DualAttention()(x)\n",
    "\n",
    "    output = decoder_full(a, x, filters, num_classes, att_indices)\n",
    "\n",
    "    model = tf.keras.Model(inp, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "resnet_unet = resnet_unet(5, 512, 3)\n",
    "resnet_unet.compile(optimizer=\"adam\", loss=dice_loss, metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficientnet U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.efficientnet import EfficientNetB4, preprocess_input\n",
    "\n",
    "def effnet_stem(input, layers):\n",
    "\n",
    "    x = input\n",
    "\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    \n",
    "    return x\n",
    "    \n",
    "def effblock_enc(inp, layers, model):\n",
    "\n",
    "    enc = tf.keras.Model(inputs=model.get_layer(layers[0].name).input, outputs=model.get_layer(layers[-1].name).output)\n",
    "    x = enc(inp)\n",
    "    return x\n",
    "\n",
    "def effnet_encoder(inp, layer_dict, model):\n",
    "\n",
    "    keys = list(layer_dict.keys())\n",
    "\n",
    "    x = effnet_stem(inp, layer_dict[keys[0]])\n",
    "\n",
    "    activations = [x]\n",
    "    for block in keys[1:]:\n",
    "\n",
    "        x = effblock_enc(x, layer_dict[block], model) \n",
    "        if block == \"block_1\" or block ==  \"block_4\" or block == \"block_6\":\n",
    "            continue\n",
    "        activations.append(x)\n",
    "    \n",
    "    return x, [None] + activations[:-1]\n",
    "\n",
    "def effnet_unet(num_classes, input_size, input_dim, att_indices=[], last_attention=False):\n",
    "\n",
    "    b4 = EfficientNetB4(weights=\"imagenet\", include_top=False, input_shape=(input_size,input_size,input_dim))\n",
    "\n",
    "    effnet_blocks_dict = {}\n",
    "    stem_start = [b4.layers[0], b4.layers[1], b4.layers[2]]\n",
    "    stem_start.extend([layer for layer in b4.layers if layer.name[:4] == \"stem\"])\n",
    "    effnet_blocks_dict[\"stem\"] = stem_start\n",
    "    effnet_blocks_dict = {**effnet_blocks_dict, **{ \n",
    "        f\"block_{n}\" : [layer for layer in b4.layers if layer.name[:6] == f\"block{n}\"] for n in range(1, 8)\n",
    "    }}\n",
    "\n",
    "    #Freeszing the weights of the model\n",
    "    b4.trainable = False\n",
    "\n",
    "    #Encoder of the network\n",
    "    inp = tf.keras.layers.Input((input_size, input_size, input_dim))\n",
    "    x = preprocess_input(inp)\n",
    "    #a not being reversed here because it's reversed in the decoder function\n",
    "    x, a = effnet_encoder(x, effnet_blocks_dict, b4)\n",
    "\n",
    "    #Decoder of the network\n",
    "    filters = [160, 56, 32, 48, 64]\n",
    "\n",
    "    l = len(att_indices)\n",
    "    assert l >= 0, \"Attention indices should be 0 or greater\"\n",
    "    assert l <= len(filters) + 1, \"Number of layers for attetention can not exceed 5\"\n",
    "\n",
    "    #assert len(att_indices[att_indices > 5]) == 0, \"Attention indices must be from 1 to 5\"\n",
    "\n",
    "\n",
    "    if last_attention:\n",
    "        x = DualAttention()(x)\n",
    "    \n",
    "    output = decoder_full(a, x, filters, num_classes, att_indices)\n",
    "\n",
    "    model  = tf.keras.Model(inp, output)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "eff_unet = effnet_unet(5, 512, 3)\n",
    "eff_unet.compile(optimizer=\"adam\", loss=dice_loss, metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data paths, and dataset definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The directory where the images and the segmentation maps are orignally stored - the zip files\n",
    "og_img_dir = \"\"\n",
    "og_map_dir = \"\"\n",
    "\n",
    "#The directory where the images and segmentation maps are stored they need to be completed based on where the model is trained, this directory is where the train, test, and validation directories are stored\n",
    "image_dir = \"\"\n",
    "segmaps_dir = \"\"\n",
    "\n",
    "#The directory for the training images and segmentation maps\n",
    "train_img_dir = \"\"\n",
    "train_map_dir = \"\"\n",
    "\n",
    "#The directory for the test images and segmentation maps\n",
    "test_img_dir = \"\"\n",
    "test_map_dir = \"\"\n",
    "\n",
    "#The directory for the validation images and segmentation maps\n",
    "val_img_dir = \"\"\n",
    "val_map_dir = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#File seperation into train, test, and validation sets\n",
    "fs = SegmentationFileSeperator(image_dir, og_img_dir, segmaps_dir, og_map_dir)\n",
    "fs.run(0.7, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SegmentationDataSet(train_img_dir, train_map_dir, (512, 512), 5)\n",
    "test_ds = SegmentationDataSet(test_img_dir, test_map_dir, (512, 512), 5)\n",
    "valid_ds = SegmentationDataSet(val_img_dir, val_map_dir, (512, 512), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db96c7c19d6ddb955ec6ab14ba350af8085dd720a16eea0a08edd8c99e1901ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
